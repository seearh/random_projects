Logs Generator - This kdb process generates mocked trade logs in the specified format and writes to a log file with the specified naming convention.

1. In a new session, set environment variables
	TRADE_LOG_CID=1
	TRADE_LOG_DIR=<desired directory to set log file>
		e.g. TRADE_LOG_DIR=c:/Users/congr/Desktop/Python/random_projects/citi_kdb/logs

2. Run 
	q client_logger.q
to start up this process.

3. Once started up, the log file for the day (and necessary parent directories) will be created. The process has been configured to log between 10 and 20 entries per second.

DESCRIPTION
This is a simplified implementation that only mocks the fields that will be aggregated by the downstream main server (i.e. other fields have unchanging values). Log messages are generated from a template string with placeholders. An internal trade table holds records to be logged at specified times.

At start-up, a handle is opened to a log file (created if it doesn't exist). The log file name is constructed with the current date and the client ID. The log directory and client ID are obtained through environment variables. Roughly 10 seconds worth of records are inserted into the trade table. 

Everytime the timer ticks (0.1s), the process:
- checks if a new day has started. If it has, it rolls over to the next day's log file.
- writes trade table records onto the log file by mapping each row into the log template's placeholders based on its column names, then removes those records.
- replenishes the trade table with 10 seconds worth of records when it has <3 seconds worth of records left.



Client Feed - This kdb process routinely monitors the size of the log file and publishes any appended log entries to the Main Server.

1. In a new session, set environment variables
	TRADE_LOG_CID=1
	TRADE_LOG_DIR=<desired directory to set log file>
		e.g. TRADE_LOG_DIR=c:/Users/congr/Desktop/Python/random_projects/citi_kdb/logs
This should be identical to the ones used in the Logs Generator session.

2. Run
	q client_feed.q :5050
to start up the process. The main server handle can be provided as a command line argument in the format [host]:port[:user:pwd].

3. Once started up, it should have registered the cursor (which is used as an offset) of the existing log file, and it will constantly attempt to connect to the Main Server. If the Main Server is restarted, run
	connectToServer`
in the process to re-establish connection.

DESCRIPTION
This is a log parser process, as well as a publishing client which connects to the main server. It contains a mapping specification that casts and renames each column accordingly, after parsing appended log records as key-value pairs.

At start-up, it:
- looks for the log file written to by the Logs Generator, similarly using the current date, client ID and logs directory (environment variables), obtaining its file size and maintaining it as a global variable.
- attempts to connect to the Main Server, whose connection handle is constructed from the command line argument.

Everytime the timer ticks (0.1s), the process:
- looks for the log file, if it wasn't found at first.
- retries for Main Server connection, if it wasn't successful at first.
- publishes the entirety of yesterday's log file and rollover to the next day's log file, if the day has passed.
- parses any appends made to the log file since the previous tick, then asynchronously calls the update analytic on the Main Server with it to publish it into its `trade table.

It can be extended to parse and publish multiple log files according to regexes. Chunking logic can also be implemented to place a cap on the size of each published message. The reconnection logic can be improved.



Main Server - This kdb process receives log entries from (potentially many) Client Feed. It routinely splays the summary report, keeping only 1 entry per AccountID and Symbol in-memory. The concise summary report is available in real-time through the websockets.html file.

1. In a new session, set environment variables 
	DB_ROOT=<desired directory to splay summary report>
		e.g. DB_ROOT=c:/Users/congr/Desktop/Python/random_projects/citi_kdb/db

2. Run
	q main_server.q -p 5050
to start up the process. The port is configurable, but relevant changes needs to be made to client_feed.q and websockets.js.

3. Once started up, the `summ table should be populated. Opening websockets.html in your browser should display the real-time `summ table.

It contains a schema for the trade logs (which should be identical to that in Client Feed processes) for the sole purpose of ensuring column consistency on receipt. An update analytic is defined such that a summary report is calculated from trade entries received and merged with the existing summary report.

Everytime the timer ticks (1s), the process:
- splays the summary report under the DB_ROOT directory and appends to it subsequently, if it has been 10s since it was last splayed.
- push data to websocket connections, by running the analytic they subscribed to with provided arguments.